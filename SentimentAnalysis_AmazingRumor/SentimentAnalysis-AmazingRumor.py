# -*- coding: utf-8 -*-
"""character-network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k1YDHO8lCFI4-g1Osi8xj06ecOONVTty

# 개요

 이 프로젝트는 NCSOFT와 홍익대학교 게임학부에서 진행하는 '스토리 흥미도 과제'에서 **bottom-up 방법** 중, 다음(Daum) 웹툰 '경이로운 소문'에서 **회차별 감정분석, 캐릭터 추출 및 캐릭터 간 관계를 그래프로 표현**하기 위해 작성되었습니다.

 이 코드는 JICS 논문, 텍스트 스토리에서 등장인물간 감정 흐름 그래프를 이용한
행위소 모델 기반의 등장인물 역할 인식(유혜연, 김문현, 배병철.)을 참고하여 행위소 모델 기반 주인공(subject), 대립자(opponent), 협력자(helper), 목표(object)를 자동으로 추출하는 방법을 제시합니다.
또한, 캐릭터 간 감성 분석(긍정/부정/중립)을 통해 캐릭터 관계도(network)를 나타냅니다.

**감정 분석**은 NLTK(Netrual Language Toolkit)에서 제공하는 **VADER**(Valence Aware Dictionary and Sentiment Reasoner)으로 진행됩니다.

> NLTK VADER의 특징
*   입력 텍스트 -> 각각 긍정(positive), 부정(negative), 중립(neutral), 복합값(compound)의 해당하는 실수값(-1과 1 사이값) 계산
*   임계값 범위 설정 -> 입력 텍스트 중 하나의 값을 사용할 시 복합값을 측정기준으로 사용



---

# 요약

이 코드는 아래와 같은 결과물을 제공합니다.
*   회차별 작품 내 감정 및 독자들의 감정
*   행위소 모델 기반 캐릭터 추출
*   감성 기반 캐릭터 관계도


---


*작성자 홍익대학교 게임소프트웨어전공 정해빈*

# 1. 드라마 <경이로운 소문> 감정분석

드라마 <경의로운 소문>의 대본, 네이버 톡(실시간 채팅), 다음 웹툰 댓글을 감정분석하고 해당 회차에서 어떤 감정이 얼마만큼 추출되는지 비교해봅니다.

# (1) 드라마 <경이로운 소문> 대본 데이터셋
"""

# TODO 드라마 <경이로운 소문> 시즌 1화 감정분석
# 필요한 라이브러리 import
import pandas as pd # pnadas 라이브러리
import re           # 정규식 라이브러리

# # 구글 드라이브에서 데이터셋 로드
# from google.colab import drive
#
# drive.mount('/content/drive')

# 드라마 <경이로운 소문> 시즌 1화 대본 데이터셋 경로 저장
filename = '/content/drive/My Drive/Colab Notebooks/Story Interest/경이로운 소문 시즌1_1화_KOR.txt'
data = open(filename, encoding='utf-8')
print(data.readline())

# 등장인물 및 대사 딕셔너리 생성
name_list = {}

# 대사 리스트 생성
script_list = []

# 우측부터 문자열을 지우는 함수
def replace_right(original, old, new, count_right):
    repeat=0
    text = original
    old_len = len(old)
    
    count_find = original.count(old)
    if count_right > count_find : # 바꿀 횟수가 문자열에 포함된 old보다 많다면
        repeat = count_find # 문자열에 포함된 old의 모든 개수(count_find)만큼 교체
    else :
        repeat = count_right # 아니라면 입력받은 개수(count)만큼 교체

    while repeat:
        find_index = text.rfind(old) # 오른쪽부터 index를 찾기위해 rfind 사용
        text = text[:find_index] + new + text[find_index+old_len:]

        repeat -= 1
      
    return text

# 대사 내 소리 텍스트 제거 함수
def remove_sounds(sentence, start, end):
    result = len(sentence) - (end-start) + 1
    count = end

    while len(sentence) >= result-1:
        sentence = replace_right(sentence, sentence[count], "", 1)
        count = count - 1

    return sentence

# 경이로운 소문 1화 대본 데이터셋에서 필요한 정보 추출
while True:
    row_data = data.readline()
    
    if not row_data:        
        break

    # 이름 추출
    name = row_data[row_data.find("(")+1:row_data.find(")")]    # 소괄호 내 문자열은 이름

    # 한 줄에 소괄호가 없으면 생략
    if row_data.find("(") == -1 or name == "":
        continue
        
    # 대사 추출(줄바꿈 제외)
    script = row_data[row_data.find(")")+2:row_data.find('\n')]
    
    # 대사 내에 소리 텍스트가 있다면 삭제
    while script.find("[") is not -1:
        script = remove_sounds(script, script.find("["), script.find("]"))
    
    # [디버깅] 추출 등장인물 및 대사    
    # print(name + ": " + script)

    # 딕셔너리 없는 등장인물이면 딕셔너리에 새로 추가
    if name not in name_list:
        name_list[name] = [script]
    else:
        name_list[name].append(script)

    # 대사 데이터만 추가
    script_list.append(script)

# [디버깅]추출된 등장인물 출력
# print(name_list)

"""넷플릭스 드라마 <경이로운 소문> 시즌1 1화의 대본으로 감정 분류(Emotion Classification)을 진행합니다.

GoEmotions 데이터셋을 한국어로 번역하여 KoELECTRA로 학습하는 GoEmotions-Korean을 이용했습니다.

참고 주소 링크: https://github.com/monologg/GoEmotions-Korean

"""

# 필요한 라이브러리 설치

# !pip install mxnet-cu101
# !pip install gluonnlp pandas tqdm
# !pip install sentencepiece==0.1.85
# !pip install transformers==3.5.0
# !pip install torch==1.7.0
# !pip install googletrans
# !pip install attrdict==2.0.1
!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master
!pip install git+git://github.com/ssut/py-googletrans

"""emotion multi-labeling에 필요한 클래스를 사용합니다. (GoEmotions-Korean)"""

from typing import Union, Optional

import numpy as np
from transformers.pipelines import ArgumentHandler
from transformers import (
    Pipeline,
    PreTrainedTokenizer,
    ModelCard
)


class MultiLabelPipeline(Pipeline):
    def __init__(
        self,
        model: Union["PreTrainedModel", "TFPreTrainedModel"],
        tokenizer: PreTrainedTokenizer,
        modelcard: Optional[ModelCard] = None,
        framework: Optional[str] = None,
        task: str = "",
        args_parser: ArgumentHandler = None,
        device: int = -1,
        binary_output: bool = False,
        threshold: float = 0.3
    ):
        super().__init__(
            model=model,
            tokenizer=tokenizer,
            modelcard=modelcard,
            framework=framework,
            args_parser=args_parser,
            device=device,
            binary_output=binary_output,
            task=task
        )

        self.threshold = threshold

    def __call__(self, *args, **kwargs):
        outputs = super().__call__(*args, **kwargs)
        scores = 1 / (1 + np.exp(-outputs))  # Sigmoid
        results = []
        for item in scores:
            labels = []
            scores = []
            for idx, s in enumerate(item):
                if s > self.threshold:
                    labels.append(self.model.config.id2label[idx])
                    scores.append(s)
            results.append({"labels": labels, "scores": scores})
        return results

import torch.nn as nn
from torch.nn import BCEWithLogitsLoss
from transformers.modeling_electra import ElectraModel, ElectraPreTrainedModel


class ElectraForMultiLabelClassification(ElectraPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.electra = ElectraModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)
        self.loss_fct = BCEWithLogitsLoss()

        self.init_weights()

    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            labels=None,
    ):
        discriminator_hidden_states = self.electra(
            input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds
        )
        pooled_output = discriminator_hidden_states[0][:, 0]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        outputs = (logits,) + discriminator_hidden_states[1:]  # add hidden states and attention if they are here

        if labels is not None:
            loss = self.loss_fct(logits, labels)
            outputs = (loss,) + outputs

        return outputs  # (loss), logits, (hidden_states), (attentions)

"""Go-Emotions-Korean이 잘 작동하는지 테스트합니다."""

#from multilabel_pipeline import MultiLabelPipeline
from transformers import ElectraTokenizer
#from model import ElectraForMultiLabelClassification
from pprint import pprint


tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-finetuned-goemotions")
model = ElectraForMultiLabelClassification.from_pretrained("monologg/koelectra-base-finetuned-goemotions")

goemotions = MultiLabelPipeline(
    model=model,
    tokenizer=tokenizer,
    threshold=0.3
)

texts = [
    "전혀 재미 있지 않습니다 ...",
    "나는 “지금 가장 큰 두려움은 내 상자 안에 사는 것” 이라고 말했다.",
    "곱창... 한시간반 기다릴 맛은 아님!",
    "애정하는 공간을 애정하는 사람들로 채울때",
    "너무 좋아",
    "딥러닝을 짝사랑중인 학생입니다!",
    "마음이 급해진다.",
    "아니 진짜 다들 미쳤나봨ㅋㅋㅋ",
    "개노잼"
]

pprint(goemotions(texts))

"""넷플릭스 드라마 <경이로운 소문> 시즌1 1화 대사들을 넣어 mulit-label emotions로 나타냅니다."""

# TODO 드라마 <경이로운 소문> 시즌1 1화 대사에 대한 감정 추출
result = goemotions(script_list)
#pprint(result[0:5])

# [디버그] type 확인
#print(type(script_list))
#print(type(script_list[0]))

# 대본 감정을 합산
script_emotions = {}

for dic in result:
    emo_list = dic.get('labels')
    size = len(dic.get('labels'))
    for i in range(size):
        if script_emotions.get(emo_list[i]) == None:
            script_emotions[emo_list[i]] = 1
        else:
            script_emotions[emo_list[i]] += 1
        
pprint(script_emotions)

"""결과를 그래프로 나타냅니다."""

import matplotlib.pyplot as plt
import numpy as np

rank = 0

emotion = list(script_emotions.keys())    # x축: 감정의 종류
count = list(script_emotions.values())    # y축: 횟수

x = np.arange(len(script_emotions))

## 수평 막대그래프
#plt.barh(x, count, height=0.5, align='edge', tick_label=emotion, log=False)

## 원그래프
#plt.pie(count, labels = emotion, autopct = "%.1f%%")
# plt.xlabel('counts')
# plt.ylabel('emotion type')

#수직 막대그래프
plt.bar(x, count)
plt.xticks(x, emotion, rotation = 90)
plt.xlabel('emotion type')
plt.ylabel('counts')

plt.title('Emotion Counts of The Uncanny Counter Ep1')
plt.figure(figsize=(20, 30))
plt.show()

"""# (2) 드라마 <경이로운 소문> 네이버 톡 데이터셋


*   데이터셋은 sentence(내용), date(날짜_연-월-일), time(시간)
*   드라마 내용에 대한 감상, 사용자 간의 소통, 외국어 포함 (Ex. 테레슈님 也会说中文?(중국어 할 수 있습니까?) / KANGNING님 아주 조금이요^^)
*   비속어 및 은어, 언급(사용자 간의 대화), 외국어에 대한 처리는 별도로 하지 않음
*   [주의] 인종차별적 발언 있음(3707행-Chinese Ah! Get out of here!, 3718행-Chinese!! Get out of here! 등)

[예외 처리 조건]
1. 데이터 셋에서 시간과 내용이 정확히 동일한 중복되는 행은 제외



[방영 시간표]

1회 - 2020-11-28
"""

# # TODO 드라마 <경이로운 소문> 시즌 1화 네이버톡(talk) 감정분석
filename = '/content/drive/My Drive/Colab Notebooks/Story Interest/rumor_netflix_talk.csv'
df = pd.read_csv(filename, encoding='utf-8')

# 중복되는 행 삭제
df = df.drop_duplicates(["sentence", "time"], keep="first")
df

# 감정분석할 열(sentence) 추출
# 1회차 실시간 톡(talk) 추출
talk_list = []
ep1_talks = df[df['date'] == "2020-11-28"]
col_sen = ep1_talks['sentence'].astype(str)
talk_list = col_sen.tolist()

# [다버그] 추출 내용 출력
pprint(talk_list[0:5])
print(type(talk_list))
print(type(talk_list[0]))

# 드라마 <경이로운 소문> 시즌1 에 대한 감정 출력
result = goemotions(talk_list)
pprint(result[0:5])

"""# 2. 행위소 모델 기반 등장인물(캐릭터) 추출

 동일 대상을 여러 번 표현하면서 단어가 변형된 경우 **"상호 참조 문제"**가 발생할 수 있다.

 예컨대, 다음 웹툰 '경이로운 소문'에서 주인공 '소문'을 부르는 호칭으로는 [소문], [똘똘이], [다리 저는 애], [다리 아픈 애] 등이 있고, 소문의 동료 '도하나'를 부르는 호칭으로는 [도하나], [하나], [싸가지], [누나] 등 여러 호칭으로 불린다.

---

논문에서도 **이전에 표현된 명사구가 다시 등장**할 때 여러 형태로 나타면서 **단어가 변형**되는 경우를 **"참조"**라 하고, 대응하는 **참조 관계를 설명**하는 과정을 **"상호 참조 문제 해결"**이라 정의하고 있다.
 
 캐릭터를 추출하기 위해서는 상호 참조 문제를 해결하여 고유명사 및 대명사로 묘사되는 등장인물을 자동으로 식별해야 한다.

---

아래는 논문에서 등장인물 식별을 위해 가정한 내용이다.
> <등장인물 식별을 위한 가정>
1.   **등장인물**은 **명사**로 표현된다.
2.   **주요 등장인물**을 텍스트에서 **다수에 걸쳐 표현**된다.

또한, 등장인물 간의 관계를 추출하기 위한 과정은 아래와 같다.

> <정보 추출의 두 단계>
1.   명명된 객체(entity)의 **인식**: 사람, 조직, 장소와 같은 실체의 이름 포함
2.   인식된 **객체간의 관계** 추출: 객체간의 의미적 관계

시작하기에 앞서, 사용한 데이터셋은 다음과 같다.


*   데이터셋: 넷플릭스(Netflix)의 경이로운 소문 1화 대본
*   구성: 대괄호[, ]-소리 텍스트, 소괄호(, )-화자, 문장-대사
*   예) [라디오 종료음] (어린 소문) 엄마, 내가 우리 가족을 가지고 이야기책 만들었다?, (문영) 그랬어? 이야기책?
"""